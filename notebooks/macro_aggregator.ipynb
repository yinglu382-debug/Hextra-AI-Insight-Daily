# 每小时自动更新（可选）
# 在 Notebook 中运行此单元，会每小时抓取并重写 HTML，一直循环直到手动中断。

try:
    while False:  # 将 False 改为 True 启用循环
        start = time.time()
        data = scrape_all()
        html = render_html(data)
        save_html(html, OUTPUT_HTML)
        elapsed = time.time() - start
        sleep_left = max(0, 3600 - int(elapsed))
        print(f"已更新，等待 {sleep_left} 秒后再次抓取...")
        time.sleep(sleep_left)
except KeyboardInterrupt:
    print("已停止自动更新循环。")
# 运行一次生成页面
all_data = scrape_all()
html = render_html(all_data)
save_html(html, OUTPUT_HTML)
def scrape_all() -> Dict[str, List[Dict]]:
    data = {}
    for c in CATEGORIES:
        data[c] = scrape_category(c)
    return data


def save_html(html: str, out_path: str = OUTPUT_HTML) -> None:
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"写入完成：{out_path}")
from html import escape

def render_html(data: Dict[str, List[Dict]], last_updated_iso: Optional[str] = None) -> str:
    last = last_updated_iso or datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")
    sections = []
    for category in CATEGORIES:
        items = data.get(category, [])
        section_items = []
        for item in items:
            title = escape(item.get("title", ""))
            url = escape(item.get("url", ""))
            summary_html = item.get("summary_html", "")
            source = escape(item.get("source", ""))
            section_items.append(
                f"""
                <article class=\"news-item\">
                  <h3 class=\"news-title\"><a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{title}</a></h3>
                  <div class=\"news-summary\">{summary_html}</div>
                  <div class=\"news-meta\">
                    <span class=\"source\">来源：{source}</span>
                    <a class=\"origin\" href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">原文链接 ↗</a>
                  </div>
                </article>
                """
            )
        sec_html = f"""
        <section>
          <h2>一、{category} <span class=\"icon\">{ICONS.get(category, '📰')}</span></h2>
          {''.join(section_items) if section_items else '<p class=\"empty\">暂无数据</p>'}
        </section>
        """
        sections.append(sec_html)

    html = f"""
    <!doctype html>
    <html lang=\"zh-CN\">
    <head>
      <meta charset=\"utf-8\" />
      <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />
      <title>宏观经济研究 - 聚合资讯</title>
      <style>
        :root {{ --bg:#0b0f17; --card:#121826; --text:#e6eefc; --muted:#8aa0c6; --accent:#3b82f6; --border:#22304a; }}
        * {{ box-sizing: border-box; }}
        body {{ margin:0; padding:24px; font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans','PingFang SC','Hiragino Sans GB','Microsoft YaHei',sans-serif; color:var(--text); background:linear-gradient(180deg,#0b0f17,#0e1525); }}
        header {{ max-width:1080px; margin:0 auto 16px; padding:8px 8px 24px; }}
        h1 {{ font-size:28px; margin:0 0 8px; letter-spacing:.5px; }}
        .meta {{ color:var(--muted); font-size:13px; }}
        main {{ max-width:1080px; margin:0 auto; display:grid; gap:18px; }}
        section {{ background:rgba(18,24,38,.8); border:1px solid var(--border); border-radius:14px; padding:14px; backdrop-filter:blur(8px); }}
        section h2 {{ font-size:20px; margin:4px 6px 12px; display:flex; align-items:center; gap:8px; }}
        .icon {{ font-size:20px; }}
        .news-item {{ border-top:1px dashed var(--border); padding:12px 6px; }}
        .news-item:first-of-type {{ border-top:none; }}
        .news-title {{ margin:0 0 8px; font-size:16px; }}
        .news-title a {{ color:var(--text); text-decoration:none; border-bottom:1px dashed transparent; }}
        .news-title a:hover {{ color:var(--accent); border-bottom-color:var(--accent); }}
        .news-summary {{ color:#d6e1f5; line-height:1.6; font-size:15px; }}
        .news-summary p {{ margin:6px 0; }}
        .news-summary iframe, .news-summary video {{ width:100%; max-height:380px; border:none; border-radius:10px; background:#000; }}
        .news-meta {{ margin-top:8px; color:var(--muted); font-size:13px; display:flex; gap:12px; align-items:center; }}
        .news-meta a.origin {{ color:var(--muted); text-decoration:none; }}
        .news-meta a.origin:hover {{ color:var(--accent); }}
        .empty {{ color:var(--muted); padding:8px; }}
        footer {{ max-width:1080px; margin:8px auto 0; color:var(--muted); font-size:12px; text-align:center; }}
      </style>
    </head>
    <body>
      <header>
        <h1>宏观经济研究</h1>
        <div class=\"meta\">每小时自动更新 · 上次更新：{last}</div>
      </header>
      <main>
        {''.join(sections)}
      </main>
      <footer>仅供学习与参考，内容均来自各来源网站，版权归原网站所有。</footer>
    </body>
    </html>
    """
    return html
import itertools
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup


def source_from_url(url: str) -> str:
    host = urlparse(url).netloc
    if "ithome.com" in host:
        return "IT之家"
    if "sina.com.cn" in host or "smartcn.cn" in host:
        return "新浪科技"
    if "36kr.com" in host:
        return "36氪"
    if "news.cn" in host:
        return "新华网"
    return host or "来源网站"


def absolutize(href: str, base_url: str) -> str:
    if not href:
        return ""
    if href.startswith("//"):
        return "https:" + href
    if href.startswith("http://") or href.startswith("https://"):
        return href
    try:
        return urljoin(base_url, href)
    except Exception:
        return href


def first_paragraph_or_video_html(soup: BeautifulSoup) -> str:
    # Prefer meaningful first paragraph
    for p in soup.select("p"):
        text = p.get_text(strip=True)
        if text and len(text) >= 20:
            return f"<p>{text}</p>"
    # Fallback to embedded media
    iframe = soup.find("iframe")
    if iframe and iframe.get("src"):
        return (
            f"<iframe src=\"{iframe['src']}\" allow=\"autoplay; encrypted-media\" "
            f"allowfullscreen loading=\"lazy\"></iframe>"
        )
    video = soup.find("video")
    if video and video.get("src"):
        return f"<video src=\"{video['src']}\" controls></video>"
    return ""


def fetch_html_sync(url: str, timeout: float = 20.0) -> str:
    with httpx.Client(headers=DEFAULT_HEADERS, follow_redirects=True, timeout=timeout) as client:
        resp = client.get(url)
        resp.raise_for_status()
        return resp.text


def scrape_list_items(url: str, selectors: List[str]) -> List[Dict]:
    try:
        html = fetch_html_sync(url)
    except Exception:
        return []
    soup = BeautifulSoup(html, "lxml")
    items: Dict[str, Dict] = {}
    for selector in selectors:
        for a in soup.select(selector):
            title = (a.get("title") or a.get_text()).strip()
            href = a.get("href") or ""
            abs_link = absolutize(href, url)
            if title and abs_link:
                items[abs_link] = {"title": title, "url": abs_link}
    return list(items.values())[:20]


def enrich_with_summary(item: Dict) -> Dict:
    url = item["url"]
    try:
        html = fetch_html_sync(url)
        soup = BeautifulSoup(html, "lxml")
        summary_html = first_paragraph_or_video_html(soup)
    except Exception:
        summary_html = ""
    return {
        "title": item["title"],
        "url": url,
        "summary_html": summary_html,
        "source": source_from_url(url),
    }


def scrape_category(category: str) -> List[Dict]:
    tasks: List[List[Dict]] = []

    def add(u: str, sels: List[str]):
        tasks.append(scrape_list_items(u, sels))

    if category == "手机":
        add("https://mobile.ithome.com/", ["a[title]", "h2 a", "h3 a"])  
        add("https://mobile.sina.com.cn/", ["h2 a", "h3 a", "a[title]"])
        add("https://www.36kr.com/search/articles/%E6%89%8B%E6%9C%BA", ["a.article-item-title", "a[href*='/p/']"])
    elif category == "智能家电":
        add("https://www.smartcn.cn/news", [".news_list a", "h2 a", "h3 a"]) 
        add("https://www.36kr.com/search/articles/%E6%99%BA%E8%83%BD%E5%AE%B6%E7%94%B5", ["a.article-item-title", "a[href*='/p/']"]) 
    elif category == "汽车":
        add("https://www.news.cn/auto/index.html", ["h2 a", "h3 a", "a[title]"]) 
        add("https://www.36kr.com/information/travel/", ["a.article-item-title", "a[href*='/p/']"]) 
    elif category == "芯片":
        add("https://so.news.cn/#search/0/%E8%8A%AF%E7%89%87/1/0", ["h2 a", "h3 a", "a[title]"]) 
        add("https://www.ithome.com/search/%E8%8A%AF%E7%89%87.html", ["h2 a", "h3 a", "a[title]"]) 
        add("https://www.36kr.com/search/articles/%E8%8A%AF%E7%89%87", ["a.article-item-title", "a[href*='/p/']"]) 
    elif category == "操作系统":
        add("https://search.sina.com.cn/?ac=product&from=tech_index&source=tech&range=title&f_name=&col=&c=news&ie=utf-8&c=news&q=%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F", ["h2 a", "h3 a", "a[title]"]) 
        add("https://www.36kr.com/search/articles/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F", ["a.article-item-title", "a[href*='/p/']"]) 
    else:
        return []

    # Flatten and de-duplicate
    flattened = list(itertools.chain.from_iterable(tasks))
    dedup: Dict[str, Dict] = {it["url"]: it for it in flattened}
    selected = list(dedup.values())[:12]
    enriched = [enrich_with_summary(it) for it in selected]
    return enriched


def fetch_all_categories() -> Dict[str, List[Dict]]:
    return {c: scrape_category(c) for c in CATEGORIES}
import os
import re
import time
import asyncio
from datetime import datetime, timezone
from typing import Dict, List, Optional

import httpx
from bs4 import BeautifulSoup

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
)
DEFAULT_HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "zh-CN,zh;q=0.9"}

CATEGORIES = ["手机", "智能家电", "汽车", "操作系统", "芯片"]
ICONS = {"手机": "📱", "智能家电": "🏠", "汽车": "🚗", "操作系统": "💻", "芯片": "💾"}
OUTPUT_HTML = "/workspace/static/macro/index.html"

os.makedirs(os.path.dirname(OUTPUT_HTML), exist_ok=True)
# 安装依赖（在受限环境可跳过，使用已有库）
%pip -q install httpx beautifulsoup4 lxml pytz
# 宏观经济研究 · 聚合资讯（Jupyter 版本）

- 每小时自动抓取以下五个分类的资讯：手机、智能家电、汽车、操作系统、芯片。
- 每条资讯包含：原文标题（超链接）、第一段文字概括（若为视频则嵌入原视频/iframe）、原文链接与来源。
- 生成静态网页：`/workspace/static/macro/index.html`，在 Hugo/Netlify 部署后对应路径为 `/macro/`。
- 可手动运行一次生成，也可启用每小时自动更新循环。

运行顺序：从上到下依次运行到“运行一次生成页面”，若需要自动刷新，再运行“每小时自动更新（可选）”。