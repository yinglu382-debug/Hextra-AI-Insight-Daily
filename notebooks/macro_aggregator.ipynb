# æ¯å°æ—¶è‡ªåŠ¨æ›´æ–°ï¼ˆå¯é€‰ï¼‰
# åœ¨ Notebook ä¸­è¿è¡Œæ­¤å•å…ƒï¼Œä¼šæ¯å°æ—¶æŠ“å–å¹¶é‡å†™ HTMLï¼Œä¸€ç›´å¾ªç¯ç›´åˆ°æ‰‹åŠ¨ä¸­æ–­ã€‚

try:
    while False:  # å°† False æ”¹ä¸º True å¯ç”¨å¾ªç¯
        start = time.time()
        data = scrape_all()
        html = render_html(data)
        save_html(html, OUTPUT_HTML)
        elapsed = time.time() - start
        sleep_left = max(0, 3600 - int(elapsed))
        print(f"å·²æ›´æ–°ï¼Œç­‰å¾… {sleep_left} ç§’åå†æ¬¡æŠ“å–...")
        time.sleep(sleep_left)
except KeyboardInterrupt:
    print("å·²åœæ­¢è‡ªåŠ¨æ›´æ–°å¾ªç¯ã€‚")
# è¿è¡Œä¸€æ¬¡ç”Ÿæˆé¡µé¢
all_data = scrape_all()
html = render_html(all_data)
save_html(html, OUTPUT_HTML)
def scrape_all() -> Dict[str, List[Dict]]:
    data = {}
    for c in CATEGORIES:
        data[c] = scrape_category(c)
    return data


def save_html(html: str, out_path: str = OUTPUT_HTML) -> None:
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"å†™å…¥å®Œæˆï¼š{out_path}")
from html import escape

def render_html(data: Dict[str, List[Dict]], last_updated_iso: Optional[str] = None) -> str:
    last = last_updated_iso or datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")
    sections = []
    for category in CATEGORIES:
        items = data.get(category, [])
        section_items = []
        for item in items:
            title = escape(item.get("title", ""))
            url = escape(item.get("url", ""))
            summary_html = item.get("summary_html", "")
            source = escape(item.get("source", ""))
            section_items.append(
                f"""
                <article class=\"news-item\">
                  <h3 class=\"news-title\"><a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{title}</a></h3>
                  <div class=\"news-summary\">{summary_html}</div>
                  <div class=\"news-meta\">
                    <span class=\"source\">æ¥æºï¼š{source}</span>
                    <a class=\"origin\" href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥ â†—</a>
                  </div>
                </article>
                """
            )
        sec_html = f"""
        <section>
          <h2>ä¸€ã€{category} <span class=\"icon\">{ICONS.get(category, 'ğŸ“°')}</span></h2>
          {''.join(section_items) if section_items else '<p class=\"empty\">æš‚æ— æ•°æ®</p>'}
        </section>
        """
        sections.append(sec_html)

    html = f"""
    <!doctype html>
    <html lang=\"zh-CN\">
    <head>
      <meta charset=\"utf-8\" />
      <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />
      <title>å®è§‚ç»æµç ”ç©¶ - èšåˆèµ„è®¯</title>
      <style>
        :root {{ --bg:#0b0f17; --card:#121826; --text:#e6eefc; --muted:#8aa0c6; --accent:#3b82f6; --border:#22304a; }}
        * {{ box-sizing: border-box; }}
        body {{ margin:0; padding:24px; font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans','PingFang SC','Hiragino Sans GB','Microsoft YaHei',sans-serif; color:var(--text); background:linear-gradient(180deg,#0b0f17,#0e1525); }}
        header {{ max-width:1080px; margin:0 auto 16px; padding:8px 8px 24px; }}
        h1 {{ font-size:28px; margin:0 0 8px; letter-spacing:.5px; }}
        .meta {{ color:var(--muted); font-size:13px; }}
        main {{ max-width:1080px; margin:0 auto; display:grid; gap:18px; }}
        section {{ background:rgba(18,24,38,.8); border:1px solid var(--border); border-radius:14px; padding:14px; backdrop-filter:blur(8px); }}
        section h2 {{ font-size:20px; margin:4px 6px 12px; display:flex; align-items:center; gap:8px; }}
        .icon {{ font-size:20px; }}
        .news-item {{ border-top:1px dashed var(--border); padding:12px 6px; }}
        .news-item:first-of-type {{ border-top:none; }}
        .news-title {{ margin:0 0 8px; font-size:16px; }}
        .news-title a {{ color:var(--text); text-decoration:none; border-bottom:1px dashed transparent; }}
        .news-title a:hover {{ color:var(--accent); border-bottom-color:var(--accent); }}
        .news-summary {{ color:#d6e1f5; line-height:1.6; font-size:15px; }}
        .news-summary p {{ margin:6px 0; }}
        .news-summary iframe, .news-summary video {{ width:100%; max-height:380px; border:none; border-radius:10px; background:#000; }}
        .news-meta {{ margin-top:8px; color:var(--muted); font-size:13px; display:flex; gap:12px; align-items:center; }}
        .news-meta a.origin {{ color:var(--muted); text-decoration:none; }}
        .news-meta a.origin:hover {{ color:var(--accent); }}
        .empty {{ color:var(--muted); padding:8px; }}
        footer {{ max-width:1080px; margin:8px auto 0; color:var(--muted); font-size:12px; text-align:center; }}
      </style>
    </head>
    <body>
      <header>
        <h1>å®è§‚ç»æµç ”ç©¶</h1>
        <div class=\"meta\">æ¯å°æ—¶è‡ªåŠ¨æ›´æ–° Â· ä¸Šæ¬¡æ›´æ–°ï¼š{last}</div>
      </header>
      <main>
        {''.join(sections)}
      </main>
      <footer>ä»…ä¾›å­¦ä¹ ä¸å‚è€ƒï¼Œå†…å®¹å‡æ¥è‡ªå„æ¥æºç½‘ç«™ï¼Œç‰ˆæƒå½’åŸç½‘ç«™æ‰€æœ‰ã€‚</footer>
    </body>
    </html>
    """
    return html
import itertools
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup


def source_from_url(url: str) -> str:
    host = urlparse(url).netloc
    if "ithome.com" in host:
        return "ITä¹‹å®¶"
    if "sina.com.cn" in host or "smartcn.cn" in host:
        return "æ–°æµªç§‘æŠ€"
    if "36kr.com" in host:
        return "36æ°ª"
    if "news.cn" in host:
        return "æ–°åç½‘"
    return host or "æ¥æºç½‘ç«™"


def absolutize(href: str, base_url: str) -> str:
    if not href:
        return ""
    if href.startswith("//"):
        return "https:" + href
    if href.startswith("http://") or href.startswith("https://"):
        return href
    try:
        return urljoin(base_url, href)
    except Exception:
        return href


def first_paragraph_or_video_html(soup: BeautifulSoup) -> str:
    # Prefer meaningful first paragraph
    for p in soup.select("p"):
        text = p.get_text(strip=True)
        if text and len(text) >= 20:
            return f"<p>{text}</p>"
    # Fallback to embedded media
    iframe = soup.find("iframe")
    if iframe and iframe.get("src"):
        return (
            f"<iframe src=\"{iframe['src']}\" allow=\"autoplay; encrypted-media\" "
            f"allowfullscreen loading=\"lazy\"></iframe>"
        )
    video = soup.find("video")
    if video and video.get("src"):
        return f"<video src=\"{video['src']}\" controls></video>"
    return ""


def fetch_html_sync(url: str, timeout: float = 20.0) -> str:
    with httpx.Client(headers=DEFAULT_HEADERS, follow_redirects=True, timeout=timeout) as client:
        resp = client.get(url)
        resp.raise_for_status()
        return resp.text


def scrape_list_items(url: str, selectors: List[str]) -> List[Dict]:
    try:
        html = fetch_html_sync(url)
    except Exception:
        return []
    soup = BeautifulSoup(html, "lxml")
    items: Dict[str, Dict] = {}
    for selector in selectors:
        for a in soup.select(selector):
            title = (a.get("title") or a.get_text()).strip()
            href = a.get("href") or ""
            abs_link = absolutize(href, url)
            if title and abs_link:
                items[abs_link] = {"title": title, "url": abs_link}
    return list(items.values())[:20]


def enrich_with_summary(item: Dict) -> Dict:
    url = item["url"]
    try:
        html = fetch_html_sync(url)
        soup = BeautifulSoup(html, "lxml")
        summary_html = first_paragraph_or_video_html(soup)
    except Exception:
        summary_html = ""
    return {
        "title": item["title"],
        "url": url,
        "summary_html": summary_html,
        "source": source_from_url(url),
    }


def scrape_category(category: str) -> List[Dict]:
    tasks: List[List[Dict]] = []

    def add(u: str, sels: List[str]):
        tasks.append(scrape_list_items(u, sels))

    if category == "æ‰‹æœº":
        add("https://mobile.ithome.com/", ["a[title]", "h2 a", "h3 a"])  
        add("https://mobile.sina.com.cn/", ["h2 a", "h3 a", "a[title]"])
        add("https://www.36kr.com/search/articles/%E6%89%8B%E6%9C%BA", ["a.article-item-title", "a[href*='/p/']"])
    elif category == "æ™ºèƒ½å®¶ç”µ":
        add("https://www.smartcn.cn/news", [".news_list a", "h2 a", "h3 a"]) 
        add("https://www.36kr.com/search/articles/%E6%99%BA%E8%83%BD%E5%AE%B6%E7%94%B5", ["a.article-item-title", "a[href*='/p/']"]) 
    elif category == "æ±½è½¦":
        add("https://www.news.cn/auto/index.html", ["h2 a", "h3 a", "a[title]"]) 
        add("https://www.36kr.com/information/travel/", ["a.article-item-title", "a[href*='/p/']"]) 
    elif category == "èŠ¯ç‰‡":
        add("https://so.news.cn/#search/0/%E8%8A%AF%E7%89%87/1/0", ["h2 a", "h3 a", "a[title]"]) 
        add("https://www.ithome.com/search/%E8%8A%AF%E7%89%87.html", ["h2 a", "h3 a", "a[title]"]) 
        add("https://www.36kr.com/search/articles/%E8%8A%AF%E7%89%87", ["a.article-item-title", "a[href*='/p/']"]) 
    elif category == "æ“ä½œç³»ç»Ÿ":
        add("https://search.sina.com.cn/?ac=product&from=tech_index&source=tech&range=title&f_name=&col=&c=news&ie=utf-8&c=news&q=%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F", ["h2 a", "h3 a", "a[title]"]) 
        add("https://www.36kr.com/search/articles/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F", ["a.article-item-title", "a[href*='/p/']"]) 
    else:
        return []

    # Flatten and de-duplicate
    flattened = list(itertools.chain.from_iterable(tasks))
    dedup: Dict[str, Dict] = {it["url"]: it for it in flattened}
    selected = list(dedup.values())[:12]
    enriched = [enrich_with_summary(it) for it in selected]
    return enriched


def fetch_all_categories() -> Dict[str, List[Dict]]:
    return {c: scrape_category(c) for c in CATEGORIES}
import os
import re
import time
import asyncio
from datetime import datetime, timezone
from typing import Dict, List, Optional

import httpx
from bs4 import BeautifulSoup

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
)
DEFAULT_HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "zh-CN,zh;q=0.9"}

CATEGORIES = ["æ‰‹æœº", "æ™ºèƒ½å®¶ç”µ", "æ±½è½¦", "æ“ä½œç³»ç»Ÿ", "èŠ¯ç‰‡"]
ICONS = {"æ‰‹æœº": "ğŸ“±", "æ™ºèƒ½å®¶ç”µ": "ğŸ ", "æ±½è½¦": "ğŸš—", "æ“ä½œç³»ç»Ÿ": "ğŸ’»", "èŠ¯ç‰‡": "ğŸ’¾"}
OUTPUT_HTML = "/workspace/static/macro/index.html"

os.makedirs(os.path.dirname(OUTPUT_HTML), exist_ok=True)
# å®‰è£…ä¾èµ–ï¼ˆåœ¨å—é™ç¯å¢ƒå¯è·³è¿‡ï¼Œä½¿ç”¨å·²æœ‰åº“ï¼‰
%pip -q install httpx beautifulsoup4 lxml pytz
# å®è§‚ç»æµç ”ç©¶ Â· èšåˆèµ„è®¯ï¼ˆJupyter ç‰ˆæœ¬ï¼‰

- æ¯å°æ—¶è‡ªåŠ¨æŠ“å–ä»¥ä¸‹äº”ä¸ªåˆ†ç±»çš„èµ„è®¯ï¼šæ‰‹æœºã€æ™ºèƒ½å®¶ç”µã€æ±½è½¦ã€æ“ä½œç³»ç»Ÿã€èŠ¯ç‰‡ã€‚
- æ¯æ¡èµ„è®¯åŒ…å«ï¼šåŸæ–‡æ ‡é¢˜ï¼ˆè¶…é“¾æ¥ï¼‰ã€ç¬¬ä¸€æ®µæ–‡å­—æ¦‚æ‹¬ï¼ˆè‹¥ä¸ºè§†é¢‘åˆ™åµŒå…¥åŸè§†é¢‘/iframeï¼‰ã€åŸæ–‡é“¾æ¥ä¸æ¥æºã€‚
- ç”Ÿæˆé™æ€ç½‘é¡µï¼š`/workspace/static/macro/index.html`ï¼Œåœ¨ Hugo/Netlify éƒ¨ç½²åå¯¹åº”è·¯å¾„ä¸º `/macro/`ã€‚
- å¯æ‰‹åŠ¨è¿è¡Œä¸€æ¬¡ç”Ÿæˆï¼Œä¹Ÿå¯å¯ç”¨æ¯å°æ—¶è‡ªåŠ¨æ›´æ–°å¾ªç¯ã€‚

è¿è¡Œé¡ºåºï¼šä»ä¸Šåˆ°ä¸‹ä¾æ¬¡è¿è¡Œåˆ°â€œè¿è¡Œä¸€æ¬¡ç”Ÿæˆé¡µé¢â€ï¼Œè‹¥éœ€è¦è‡ªåŠ¨åˆ·æ–°ï¼Œå†è¿è¡Œâ€œæ¯å°æ—¶è‡ªåŠ¨æ›´æ–°ï¼ˆå¯é€‰ï¼‰â€ã€‚